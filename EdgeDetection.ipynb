{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection Computer Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as disp_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader `GenerateImageBatch` combines a dataset and a sampler, and provides an iterable over the given dataset. The DataLoader supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://blog.kakaocdn.net/dn/b0SVvH/btqLq2FmuEs/1hnCz8VL9wvXPKTOTXzvOk/img.jpg\" width=\"700\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataloader\n",
    "disp_image(url='https://blog.kakaocdn.net/dn/b0SVvH/btqLq2FmuEs/1hnCz8VL9wvXPKTOTXzvOk/img.jpg', \n",
    "      width=700, height=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, main attributes of data are initialized such as height and width of image and labels, their count, their indeces, batch size, input and output. Then, function `next_sample` iterates over images and labels through their indeces and shuffles them. To feed the neural network, function `next_batch` generates input and output data by collecting images and labels into batches using the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "class GenerateImageBatch:\n",
    "\n",
    "    def __init__(self, img, lab, batch_size):\n",
    "\n",
    "        self.img_h = img.shape[1]\n",
    "        self.img_w = img.shape[2]\n",
    "        self.img_count = img.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.imgs = img\n",
    "        self.labs = lab\n",
    "        self.n = img.shape[0]\n",
    "        self.indices = list(range(self.n))\n",
    "        self.cur_index = 0\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "    # next sample\n",
    "    def next_sample(self):\n",
    "        self.cur_index += 1\n",
    "        \n",
    "        if self.cur_index >= self.n:\n",
    "            self.cur_index = 0\n",
    "            random.shuffle(self.indices)\n",
    "        \n",
    "        return self.imgs[self.indices[self.cur_index]], self.labs[self.indices[self.cur_index]]\n",
    "\n",
    "    # create next batch of images and labels to feed the NN\n",
    "    def next_batch(self):\n",
    "\n",
    "        X_data = np.zeros([self.batch_size, self.img_w, self.img_h, 1])\n",
    "        Y_data = np.zeros([self.batch_size, self.img_w, self.img_h, 1])\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            img, lab = self.next_sample()\n",
    "            img = img.T\n",
    "            img = np.expand_dims(img, -1)\n",
    "            X_data[i] = img\n",
    "            lab = lab.T\n",
    "            lab = np.expand_dims(lab, -1)\n",
    "            Y_data[i] = lab\n",
    "\n",
    "        inputs = X_data\n",
    "        outputs = Y_data\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.device` enables to specify the device type responsible to load a tensor into memory. The function expects a string argument specifying the device type. It is possible to pass an ordinal like the device index. or leave it unspecified for PyTorch to use the currently available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device parameters for pytorch model and root directory\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two-dimensional convolution `nn.Conv2d` is applied over a given input where the specific shape of the input images is given in the form of size, length, width, channels, and hence the output must be in a convoluted manner is called PyTorch Conv2d. Conv2d is the function to do any changes in the convolution of two-dimensional data and it mainly pertains to an image in the system where regularizations can be applied too.\n",
    "\n",
    "A convolution operation is performed on the 2D matrix provided in the system where any operations on a matrix such as matrix inversion or MAC operation is carried out with Conv2d function in the PyTorch module. This belongs to torch.nn package in which all the neural networks functions are available thus managing the tensors and convolutions of matrices. An image is modified and made into two, so the product of these two must help in reporting the value in the output.\n",
    "\n",
    "Conv2d is added to the layers of the neural network and in PyTorch, it is an instance of the nn module. These layers become the first layers in the network with specific parameters. A number of channels of the input data to be declared in the parameters along with the number of feature maps in the above layer. Instead of a number of channels, the number of feature maps can be used as input which has to be generated in the output. It is simple as it has to import all the required libraries along with the number of strides that must be given in the code. \n",
    "\n",
    "The syntax must be like this:\n",
    "a = Conv2d(in_channels, out_channels, kernel_size=(n, n), stride, padding, bias)\n",
    "\n",
    "The following parameters are used in PyTorch Conv2d. `in_channels` are used to describe how many channels are present in the input image whereas `out_channels` are used to describe the number of channels present after convolution happened in the system. The breadth and height of the filter is provided by the kernel. The field stride explains the stride of the convolution that happens in the system. The amount of implicit paddings is controlled by the field padding where a number of points are explained in each dimension.\n",
    "\n",
    "If there is a bias happens in the result and if the user knows it beforehand, it is better to give in the code beforehand using the field bias. The output received from the Conv2d is always a tensor. `Padding_mode` is another field that explains how padding happens in the code and the default value happens to be zero. The dilation field explains the space between kernel elements and the default value is 1. Groups can be used if there are any values blocked from input to output and hence it does not appear in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.sharetechnote.com/image/Python_Pytorch_nn_Conv2D_Overview_01.png\" width=\"700\" height=\"650\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2d convolution\n",
    "disp_image(url='https://www.sharetechnote.com/image/Python_Pytorch_nn_Conv2D_Overview_01.png', \n",
    "      width=700, height=650)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian Smoothing Operator `get_gaussian_kernel` performs a weighted average of surrounding pixels based on the Gaussian distribution. It is used to remove Gaussian noise and is a realistic model of defocused lens. Sigma defines the amount of blurring. The radius slider is used to control how large the template is. Large values for sigma will only give large blurring for larger template sizes. The noise can be assumed as edges due to sudden intensity change by the edge detector. The sum of the elements in the Gaussian kernel is 1, so, the kernel should be normalized before applying as convolution to the image. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases, building a perfect model to capture all the dynamic phenomenon is not possible. To compensate for these unmodeled dynamics, process noise (w) is often added to the dynamic model. Moreover, when measurements are taken, multiple sources of errors, such as calibration errors, are inevitably included in the measurements. To account for these errors, proper measurement noise must be added to the measurement model. An estimation system including these random noises and errors is called a stochastic estimation system, which can be represented by:\\\n",
    "\\\n",
    "$ x _ { k + 1 } = f(x _ { k }, w _ { k }) $ \\\n",
    "$ y _ { k } = h(x _ { k }, v _ { k }) $ \\\n",
    "\\\n",
    "where k is the time step, x<sub>k</sub> is the system state at time step k, f(x<sub>k</sub>) is the state-dependent equation of motion, h(x<sub>k</sub>) is the state dependent measurement equation, and y<sub>k</sub> is the output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process noise and measurement noise are assumed to follow zero-mean Gaussian or normal distributions, or are at least be approximated by Gaussian distributions. Also, because the exact state is unknown, the state estimate is a random variable, usually assumed to follow Gaussian distributions. Assuming Gaussian distributions for these variables greatly simplifies the design of an estimation filter, and form the basis of the Kalman filter family.\n",
    "\n",
    "A Gaussian distribution for a random variable (x) is parametrized by a mean value μ and a covariance matrix P, which is written as x∼N(μ,P). Given a Gaussian distribution, the mean, which is also the most likely value of x, is defined by expectation (E) as:\\\n",
    "\\\n",
    "$ μ = E[x] $ \\\n",
    "\\\n",
    "The mean is also called the first moment of x about the origin. The covariance that describes of the uncertainty of x is defined by expectation (E) as:\\\n",
    "\\\n",
    "$ P = E[(x - μ)(x - μ) ^ T] $ \\\n",
    "\\\n",
    "The covariance is also called the second moment of x about its mean."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dimension of x is one, P is only a scalar. In this case, the value of P is usually denoted by σ<sup>2</sup> and called variance. The square root, σ, is called the standard deviation of x. The standard deviation has important physical meaning. The following figure shows the probability density function (which describes the likelihood that x takes a certain value) for a one-dimensional Gaussian distribution with mean equal to μ and standard deviation equal to σ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://it.mathworks.com/help/fusion/ug/normal_distribution.png\" width=\"700\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian distribution\n",
    "disp_image(url='https://it.mathworks.com/help/fusion/ug/normal_distribution.png', \n",
    "      width=700, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.projectrhea.org/rhea/images/5/5c/Math2.jpg\" width=\"600\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian filtering\n",
    "disp_image(url='https://www.projectrhea.org/rhea/images/5/5c/Math2.jpg', \n",
    "      width=600, height=250)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sobel filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sobel operator `get_sobel_kernel` performs a 2-D spatial gradient measurement on an image and so emphasizes regions of high spatial frequency that correspond to edges. Typically it is used to find the approximate absolute gradient magnitude at each point in an input grayscale image. The operator consists of a pair of 3×3 convolution kernels. One kernel is simply the other rotated by 90°."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.projectrhea.org/rhea/images/thumb/9/91/XY_Kernels.png/1125px-XY_Kernels.png\" width=\"400\" height=\"180\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sobel kernals\n",
    "disp_image(url='https://www.projectrhea.org/rhea/images/thumb/9/91/XY_Kernels.png/1125px-XY_Kernels.png', \n",
    "      width=400, height=180)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These kernels are designed to respond maximally to edges running vertically and horizontally relative to the pixel grid, one kernel for each of the two perpendicular orientations. The kernels can be applied separately to the input image, to produce separate measurements of the gradient component in each orientation (call these G<sub>x</sub> and G<sub>y</sub>). These can then be combined together to find the absolute magnitude of the gradient at each point and the orientation of that gradient. The gradient magnitudes (also known as the edge strengths) can then be determined as an Euclidean distance measure by applying the law of Pythagoras. The gradient magnitude is given by:\\\n",
    "\\\n",
    "$ |G| = \\sqrt { { G _ { x } } ^ { 2 } + { G _ { y } } ^ { 2 } } $ \\\n",
    "\\\n",
    "This equation is sometimes simplified by applying Manhattan distance measure. Typically, an approximate magnitude is computed using: \\\n",
    "\\\n",
    "$ |G| = |G _ { x }| + |G _ { y }| $ \\\n",
    "\\\n",
    "It is obvious that an image of the gradient magnitudes often indicate the edges quite clearly. However, the edges are typically broad and thus do not indicate exactly where the edges are. To make it possible to determine this, the direction of the edges must be determined and stored. The angle of orientation of the edge (relative to the pixel grid) giving rise to the spatial gradient is given by: \\\n",
    "\\\n",
    "$ Θ = arctan(G _ { y } / G _ { x }) $ \\\n",
    "\\\n",
    "In this case, orientation Θ is taken to mean that the direction of maximum contrast from black to white runs from left to right on the image, and other angles are measured anti-clockwise from this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of Non-maximum suppression defined by `set_local_maxima` and `get_local_maxima` is to convert the \"blurred\" edges in the image of the gradient magnitudes to \"sharp\" edges. Basically this is done by preserving all local maxima in the gradient image, and deleting everything else. The algorithm is for each pixel in the gradient image:\n",
    "1. Round the gradient direction Θ to nearest 45◦, corresponding to the use of an 8-connected neighbourhood.\n",
    "2. Compare the edge strength of the current pixel with the edge strength of the pixel in the positive and negative gradient direction. I.e. if the gradient direction is north (Θ = 90◦), compare with the pixels to the north and south.\n",
    "3. If the edge strength of the current pixel is largest, preserve the value of the edge strength. If not, suppress (i.e. remove) the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/v2/resize:fit:547/1*CWrXNSbe7s4qSFr5vylyvQ.png\" width=\"600\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# algorithm goes through all the points on the gradient intensity matrix \n",
    "# and finds the pixels with the maximum value in the edge directions\n",
    "disp_image(url='https://cdn-images-1.medium.com/v2/resize:fit:547/1*CWrXNSbe7s4qSFr5vylyvQ.png', \n",
    "      width=600, height=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper left corner red box present on the above image, represents an intensity pixel of the Gradient Intensity matrix being processed. The corresponding edge direction is represented by the orange arrow with an angle of -pi radians (+/-180 degrees)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/v2/resize:fit:402/1*K-gnZg4_VPk57Xs0XflIrg.png\" width=\"350\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# focus on the upper left corner red box pixel\n",
    "disp_image(url='https://cdn-images-1.medium.com/v2/resize:fit:402/1*K-gnZg4_VPk57Xs0XflIrg.png', \n",
    "      width=350, height=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edge direction is the orange dotted line (horizontal from left to right). The purpose of the algorithm is to check if the pixels on the same direction are more or less intense than the ones being processed. In the example above, the pixel (i, j) is being processed, and the pixels on the same direction are highlighted in blue (i, j-1) and (i, j+1). If one those two pixels are more intense than the one being processed, then only the more intense one is kept. Pixel (i, j-1) seems to be more intense, because it is white (value of 255). Hence, the intensity value of the current pixel (i, j) is set to 0. If there are no pixels in the edge direction having more intense values, then the value of the current pixel is kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/v2/resize:fit:373/1*QjoPwEgQ6NOynOwxc4847A.png\" width=\"320\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# supress non-maximum values\n",
    "disp_image(url='https://cdn-images-1.medium.com/v2/resize:fit:373/1*QjoPwEgQ6NOynOwxc4847A.png', \n",
    "      width=320, height=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the direction is the orange dotted diagonal line. Therefore, the most intense pixel in this direction is the pixel (i-1, j+1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, each pixel has 2 main criteria (edge direction in radians, and pixel intensity (between 0–255)). Based on these inputs the non-max suppression steps are:\n",
    "\n",
    "- Create a matrix initialized to 0 of the same size of the original gradient intensity matrix;\n",
    "- Identify the edge direction based on the angle value from the angle matrix;\n",
    "- Check if the pixel in the same direction has a higher intensity than the pixel that is currently processed;\n",
    "- Return the image processed with the non-max suppression algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double threshold"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The edge-pixels remaining after the non-maximum suppression step are (still) marked with their strength pixel-by-pixel. Many of these will probably be true edges in the image, but some may be caused by noise or color variations for instance due to rough surfaces. The simplest way to discern between these would be to use a `threshold`, so that only edges stronger that a certain value would be preserved. The Canny edge detection algorithm uses double thresholding. Edge pixels stronger than the high threshold are marked as strong; edge pixels weaker than the low threshold are suppressed and edge pixels between the two thresholds are marked as weak."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strong edges are interpreted as \"certain edges\", and can immediately be included in the final\n",
    "edge image. Weak edges are included if and only if they are connected to strong edges. The logic is of course that noise and other small variations are unlikely to result in a strong edge\n",
    "(with proper adjustment of the threshold levels). Thus strong edges will (almost) only be due to\n",
    "true edges in the original image. The weak edges can either be due to true edges or noise/color\n",
    "variations. The latter type will probably be distributed independently of edges on the entire\n",
    "image, and thus only a small amount will be located adjacent to strong edges. Weak edges due\n",
    "to true edges are much more likely to be connected directly to strong edges."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Tracking by Hysteresis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to some edges are more bright than others, the brighter ones can be considered as strong edges but the lighter ones can actually be edges or they can be because of noise. To solve the problem of which edges are really edges and which are not Canny uses the Hysteresis thresholding. In this, two thresholds are set to \"High\" and \"Low\". Any edges with intensity greater than \"High\" are the sure edges. Any edges with intensity less than \"Low\" are sure to be non-edges. The edges between \"High\" and \"Low\" thresholds are classified as edges only if they are connected to a sure edge otherwise discarded.\n",
    "\n",
    "Function `hysteresis` is the lagging of an effect — a kind of inertia. In the context of thresholding, it means that areas above some low threshold are considered to be above the threshold if they are also connected to areas above a higher, more stringent, threshold. They can thus be seen as continuations of these high-confidence areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i0.wp.com/theailearner.com/wp-content/uploads/2019/05/Hysteresis-2.png?w=379&ssl=1\" width=\"350\" height=\"250\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hysteresis thresholding\n",
    "disp_image(url='https://i0.wp.com/theailearner.com/wp-content/uploads/2019/05/Hysteresis-2.png?w=379&ssl=1', \n",
    "      width=350, height=250)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, \"A\" and \"B\" are sure-edges as they are above \"High\" threshold. Similarly, \"D\" is a sure non-edge. Both \"E\" and \"C\" are weak edges but since \"C\" is connected to \"B\" which is a sure edge, \"C\" is also considered as a strong edge. Using the same logic \"E\" is discarded. This way we will get only the strong edges in the image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the threshold results, the hysteresis consists of transforming weak pixels into strong ones, if and only if at least one of the pixels around the one being processed is a strong one, as described below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn-images-1.medium.com/v2/resize:fit:675/1*jnqS5hbRwAmU-sgK552Mgg.png\" width=\"600\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# edge tracking\n",
    "disp_image(url='https://cdn-images-1.medium.com/v2/resize:fit:675/1*jnqS5hbRwAmU-sgK552Mgg.png', \n",
    "      width=600, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, batch_size, k_gaussian=3, k_sobel=3, mu=0, sigma=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # gaussian filter\n",
    "        gaussian_2D = self.get_gaussian_kernel(k_gaussian, mu, sigma)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=k_gaussian,\n",
    "            padding=k_gaussian//2,\n",
    "            padding_mode='replicate',\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        gaussian_2D = torch.from_numpy(gaussian_2D)\n",
    "        gaussian_2D.requires_grad = True\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.conv1.weight[:] = gaussian_2D\n",
    "\n",
    "        # sobel filter x direction\n",
    "        sobel_2D = self.get_sobel_kernel(k_sobel)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=k_sobel,\n",
    "            padding=k_sobel//2,\n",
    "            padding_mode='replicate',\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        sobel_2D = torch.from_numpy(sobel_2D)\n",
    "        sobel_2D.requires_grad = True\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.conv2.weight[:] = sobel_2D\n",
    "\n",
    "        # sobel filter y direction\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=k_sobel,\n",
    "            padding=k_sobel//2,\n",
    "            padding_mode='replicate',\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.conv3.weight[:] = sobel_2D.T\n",
    "\n",
    "        # hysteresis custom kernel\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            padding_mode='replicate',\n",
    "            bias=False\n",
    "        ).to(device)\n",
    "        \n",
    "        hyst_kernel = np.ones((3, 3)) + 0.25\n",
    "        hyst_kernel = torch.from_numpy(hyst_kernel).unsqueeze(0).unsqueeze(0)\n",
    "        hyst_kernel.requires_grad = False\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.conv4.weight = nn.Parameter(hyst_kernel)\n",
    "\n",
    "        # threshold parameters\n",
    "        self.low_threshold = torch.nn.Parameter(torch.tensor(0.10), requires_grad=True)\n",
    "        self.high_threshold = torch.nn.Parameter(torch.tensor(0.20), requires_grad=True)\n",
    "\n",
    "    def get_gaussian_kernel(self, k=3, mu=0, sigma=1, normalize=True):\n",
    "        \n",
    "        # compute 1-dimension gaussian\n",
    "        gaussian_1D = np.linspace(-1, 1, k)\n",
    "        \n",
    "        # compute grid distance from center\n",
    "        x, y = np.meshgrid(gaussian_1D, gaussian_1D)\n",
    "        distance = (x ** 2 + y ** 2) ** 0.5\n",
    "        \n",
    "        # compute 2-dimension gaussian\n",
    "        gaussian_2D = np.exp(-(distance - mu) ** 2 / (2 * sigma ** 2))\n",
    "        gaussian_2D = gaussian_2D / (2 * np.pi * sigma ** 2)\n",
    "\n",
    "        # normalize part (mathematically)\n",
    "        if normalize:\n",
    "            gaussian_2D = gaussian_2D / np.sum(gaussian_2D)\n",
    "        \n",
    "        return gaussian_2D\n",
    "\n",
    "    def get_sobel_kernel(self, k=3):\n",
    "        \n",
    "        # get range\n",
    "        range = np.linspace(-(k // 2), k // 2, k)\n",
    "        \n",
    "        # compute a grid, the numerator and the axis-distances\n",
    "        x, y = np.meshgrid(range, range)\n",
    "        sobel_2D_numerator = x\n",
    "        sobel_2D_denominator = (x ** 2 + y ** 2)\n",
    "        sobel_2D_denominator[:, k // 2] = 1  # avoid division by zero\n",
    "        sobel_2D = sobel_2D_numerator / sobel_2D_denominator\n",
    "        \n",
    "        return sobel_2D\n",
    "\n",
    "    # get the magnitudes shifted left to make a matrix of the points to the right of pts; \n",
    "    # similarly, shift left and down to get the points to the top right of pts.\n",
    "    def set_local_maxima(self, magnitude, pts, w_num, w_denum, row_slices, col_slices, out):\n",
    "        \n",
    "        pts = pts.to(device)\n",
    "        out = out.to(device)\n",
    "        \n",
    "        # 1d vectors of pixels (r_0 = [x1, x2...xn])\n",
    "        r_0, r_1, r_2, r_3 = row_slices\n",
    "        c_0, c_1, c_2, c_3 = col_slices        \n",
    "      \n",
    "        # magnitude of pixel gradients at each point\n",
    "        m = magnitude[pts]\n",
    "\n",
    "        # change of pixel magnitude between sobel-x and sobel-y images\n",
    "        w = w_num[pts] / w_denum[pts]\n",
    "\n",
    "        # calculate edge strength of the pixel in positive direction \n",
    "        c1 = magnitude[:, 0, r_0, c_0][pts[:, 0, r_1, c_1]]\n",
    "        c2 = magnitude[:, 0, r_2, c_2][pts[:, 0, r_3, c_3]]\n",
    "        c_plus = c2 * w + c1 * (1 - w) <= m\n",
    "        c_plus = c_plus.to(device)\n",
    "        \n",
    "        # calculate edge strength of the pixel in negative direction\n",
    "        c1 = magnitude[:, 0, r_1, c_1][pts[:, 0, r_0, c_0]]\n",
    "        c2 = magnitude[:, 0, r_3, c_3][pts[:, 0, r_2, c_2]]\n",
    "        c_minus = c2 * w + c1 * (1 - w) <= m\n",
    "        c_minus = c_minus.to(device)\n",
    "\n",
    "        # bitwise addition of edge strength of the pixel \n",
    "        # in positive and negative gradient direction         \n",
    "        out[pts] = c_plus & c_minus\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    # edge thinning by non-maximum suppression\n",
    "    def get_local_maxima(self, isobel, jsobel, magnitude, eroded_mask):\n",
    "\n",
    "        # computes absolute value of each element\n",
    "        abs_isobel = torch.abs(jsobel)\n",
    "        abs_jsobel = torch.abs(isobel)\n",
    "\n",
    "        # filter eroded mask by negative values\n",
    "        eroded_mask = eroded_mask & (magnitude > 0)\n",
    "\n",
    "        # normals' orientations\n",
    "        is_horizontal = eroded_mask & (abs_isobel >= abs_jsobel)\n",
    "        is_vertical = eroded_mask & (abs_isobel <= abs_jsobel)\n",
    "        is_up = (isobel >= 0)\n",
    "        is_down = (isobel <= 0)\n",
    "        is_right = (jsobel >= 0)\n",
    "        is_left = (jsobel <= 0)\n",
    "        \n",
    "        # --------- Find local maxima --------------\n",
    "        #\n",
    "        # Assign each point to have a normal of 0-45 degrees, 45-90 degrees,\n",
    "        # 90-135 degrees and 135-180 degrees.\n",
    "        #\n",
    "        local_maxima = torch.zeros(magnitude.shape, dtype=bool)\n",
    "        \n",
    "        # ----- 0 to 45 degrees ------\n",
    "        # mix diagonal and horizontal\n",
    "        pts_plus = is_up & is_right\n",
    "        pts_minus = is_down & is_left\n",
    "        pts = ((pts_plus | pts_minus) & is_horizontal)\n",
    "        \n",
    "        # Get the magnitudes shifted left to make a matrix of the points to the\n",
    "        # right of pts. Similarly, shift left and down to get the points to the\n",
    "        # top right of pts.\n",
    "        local_maxima = self.set_local_maxima(\n",
    "            magnitude, pts, abs_jsobel, abs_isobel,\n",
    "            [slice(1, None), slice(-1), slice(1, None), slice(-1)],\n",
    "            [slice(None), slice(None), slice(1, None), slice(-1)],\n",
    "            local_maxima\n",
    "        )\n",
    "        \n",
    "        # ----- 45 to 90 degrees ------\n",
    "        # mix diagonal and vertical\n",
    "        pts = ((pts_plus | pts_minus) & is_vertical)\n",
    "        local_maxima = self.set_local_maxima(\n",
    "            magnitude, pts, abs_isobel, abs_jsobel,\n",
    "            [slice(None), slice(None), slice(1, None), slice(-1)],\n",
    "            [slice(1, None), slice(-1), slice(1, None), slice(-1)],\n",
    "            local_maxima\n",
    "        )\n",
    "        \n",
    "        # ----- 90 to 135 degrees ------\n",
    "        # mix anti-diagonal and vertical\n",
    "        pts_plus = is_down & is_right\n",
    "        pts_minus = is_up & is_left\n",
    "        pts = ((pts_plus | pts_minus) & is_vertical)\n",
    "        local_maxima = self.set_local_maxima(\n",
    "            magnitude, pts, abs_isobel, abs_jsobel,\n",
    "            [slice(None), slice(None), slice(-1), slice(1, None)],\n",
    "            [slice(1, None), slice(-1), slice(1, None), slice(-1)],\n",
    "            local_maxima\n",
    "        )\n",
    "        \n",
    "        # ----- 135 to 180 degrees ------\n",
    "        # mix anti-diagonal and anti-horizontal\n",
    "        pts = ((pts_plus | pts_minus) & is_horizontal)\n",
    "        local_maxima = self.set_local_maxima(\n",
    "            magnitude, pts, abs_jsobel, abs_isobel,\n",
    "            [slice(-1), slice(1, None), slice(-1), slice(1, None)],\n",
    "            [slice(None), slice(None), slice(1, None), slice(-1)],\n",
    "            local_maxima\n",
    "        )\n",
    "\n",
    "        return local_maxima\n",
    "\n",
    "    # thresholds for defining weak and strong edge pixels\n",
    "    def threshold(self, img):\n",
    "    \n",
    "        alpha = 100000\n",
    "        weak = 0.5\n",
    "        strong = 1\n",
    "        \n",
    "        res_strong = strong * (alpha * (img - self.high_threshold)).sigmoid()\n",
    "        res_weak_1 = weak * (alpha * (self.high_threshold - img)).sigmoid()\n",
    "        res_weak_2 = weak * (alpha * (self.low_threshold - img)).sigmoid()\n",
    "        res_weak = res_weak_1 - res_weak_2\n",
    "        res = res_weak + res_strong\n",
    "\n",
    "        return res\n",
    "\n",
    "    def hysteresis(self, img):\n",
    "\n",
    "        # create image that has strong pixels remain at one, weak pixels become zero\n",
    "        img_strong = img.clone()\n",
    "        img_strong[img == 0.5] = 0\n",
    "\n",
    "        # create masked image that turns all weak pixel into ones, rest to zeros\n",
    "        masked_img = img.clone()\n",
    "        masked_img[torch.logical_not(img == 0.5)] = 0\n",
    "        masked_img[img == 0.5] = 1\n",
    "\n",
    "        # calculate weak edges that are changed to strong edges\n",
    "        changed_edges = img.clone()\n",
    "        changed_edges[((self.conv4(img_strong) > 1) & (masked_img == 1))] = 1\n",
    "\n",
    "        # add changed edges to already good edges\n",
    "        changed_edges[changed_edges != 1] = 0\n",
    "\n",
    "        return changed_edges\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Gaussian filter\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        # Sobel filter\n",
    "        sobel_x = self.conv2(x)\n",
    "        sobel_y = self.conv3(x)\n",
    "\n",
    "        # magnitude and angles\n",
    "        eps = 1e-10\n",
    "        self.grad_magnitude = torch.hypot(sobel_x + eps, sobel_y + eps)\n",
    "\n",
    "        # non-max suppression\n",
    "        eroded_mask = torch.ones((self.batch_size, 1, 256, 256), dtype=bool).to(device)\n",
    "        eroded_mask[:, 0, :1, :] = 0\n",
    "        eroded_mask[:, 0, -1:, :] = 0\n",
    "        eroded_mask[:, 0, :, :1] = 0\n",
    "        eroded_mask[:, 0, :, -1:] = 0\n",
    "        thin_edges = self.get_local_maxima(sobel_x, sobel_y, self.grad_magnitude, eroded_mask)\n",
    "        thin_edges = self.grad_magnitude * (thin_edges * 1)\n",
    "\n",
    "        # double threshold\n",
    "        thin_edges = thin_edges / torch.max(thin_edges)\n",
    "        thresh = self.threshold(thin_edges)\n",
    "\n",
    "        # hysteresis\n",
    "        result = self.hysteresis(thresh)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Net\n",
    "import numpy as np\n",
    "from batch_generator import GenerateImageBatch\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network training is the process of teaching a neural network to perform a task. Neural networks learn by initially processing several large sets of labeled data. By using the model built, they can then process unknown inputs with regard to loss function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dice score is used to quantify the performance of image segmentation. The algorithm is validated by calculating the Dice score, which is a measure of how similar the objects are. So it is the size of the overlap of the two segmentations divided by the total size of the two objects. Using the same terms as describing accuracy, the Dice score is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.mdpi.com/electronics/electronics-09-01199/article_deploy/html/images/electronics-09-01199-g0A1-550.jpg\" width=\"700\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dice score\n",
    "disp_image(url='https://www.mdpi.com/electronics/electronics-09-01199/article_deploy/html/images/electronics-09-01199-g0A1-550.jpg', \n",
    "      width=700, height=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "$ Dice = { { 2 * TP } \\over { (TP + FP) + (TP + FN) } } $ \\\n",
    "\\\n",
    "The number of true positives, is the number that method finds, the number of positives is the total number of positives that can be found and the number of false positives is the number of points that are negative that the method classifies as positive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dice score is not only a measure of how many positives you find, but it also penalizes for the false positives that the method finds, similar to precision, so it is more similar to precision than accuracy. The only difference is the denominator, thete is the total number of positives instead of only the positives that the method finds. So the Dice score is also penalizing for the positives that the algorithm/method could not find.\n",
    "\n",
    "In the case of image segmentation, there is a mask with ground truth (A). The mask has values 1 in the pixels where there are edges that have to be found and else zero. Likewise, there is an algorithm to generate image/mask (B), which also has to be a binary image, i.e. a mask is created for segmentation. Then there are following statements:\n",
    "\n",
    "- `Number of positives` is the total number of pixels that have intensity 1 in image A.\n",
    "- `Number of true positives` is the total number of pixels which have the value 1 in both A and B, so it is the intersection of the regions of ones in A and B. It is the same as using the AND operator on A and B.\n",
    "- `Number of false positives` is the number of pixels which appear as 1 in B but zero in A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate algorithm efficiency\n",
    "def dice_loss(input, target):\n",
    "    smooth = 1.\n",
    "    iflat = input.view(-1)\n",
    "    tflat = target.view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "\n",
    "    return (1 - ((2. * intersection + smooth) / (iflat.sum() + tflat.sum() + smooth)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole cycle of machine learning process includes forward and backward passes. The \"forward pass\" refers to calculation process, values of the output layers from the inputs data. It's traversing through all neurons from first to last layer. A loss function (Dice score) is calculated from the output values.\n",
    "\n",
    "And then \"backward pass\" refers to process of counting changes in weights (de facto learning), using gradient descent algorithm. Computation is made from last layer, backward to the first layer.\n",
    "\n",
    "Backward and forward pass makes together one \"iteration\".\n",
    "\n",
    "During one iteration, a subset of the data set is passed (batch). Every \"Epoch\" the entire data set is passing in batches. One epoch contains (number_of_items / batch_size) iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation of the model\n",
    "def main():\n",
    "\n",
    "    # load data\n",
    "    train_images = np.load(r'path\\train_image.npy')\n",
    "    train_labels = np.load(r'path\\train_label.npy')\n",
    "\n",
    "    valid_images = train_images\n",
    "    valid_labels = train_labels\n",
    "\n",
    "    # create batch architecture\n",
    "    batch_size = 1\n",
    "    epochs = 50\n",
    "    learning_rate = 0.00001 \n",
    "    train_data = GenerateImageBatch(train_images, train_labels, batch_size)\n",
    "    valid_data = GenerateImageBatch(valid_images, valid_labels, batch_size)\n",
    "\n",
    "    # create model object\n",
    "    net = Net(batch_size)\n",
    "    net.to(device)\n",
    "\n",
    "    net = net.float()\n",
    "\n",
    "    # set up the optimizer, the loss, the learning rate scheduler \n",
    "    # and the loss scaling for AMP\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    criterion = dice_loss\n",
    "\n",
    "    # begin training\n",
    "    for epoch in range(1, epochs+1):\n",
    "        \n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for i in range(int(train_data.img_count / batch_size)):\n",
    "            \n",
    "            # forward pass: compute predicted 'y' by passing 'x' to the model\n",
    "            train_data.next_batch()\n",
    "\n",
    "            inputs = train_data.inputs\n",
    "            inputs.astype(np.float32)\n",
    "            inputs = np.transpose(inputs, (0, 3, 1, 2))\n",
    "            inputs = torch.from_numpy(inputs)\n",
    "\n",
    "            outputs = torch.from_numpy(train_data.outputs.astype(np.float32))\n",
    "            outputs = np.transpose(outputs.float(), (0, 3, 1, 2))\n",
    "\n",
    "            inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "            \n",
    "            inputs = inputs.type(torch.float32)\n",
    "            outputs = outputs.type(torch.float32)\n",
    "\n",
    "            y_pred = net(inputs)\n",
    "            y_pred = y_pred.to(device)\n",
    "\n",
    "            # compute and print loss\n",
    "            loss = criterion(y_pred, outputs)\n",
    "\n",
    "            writer.add_scalar('Loss/train', loss, epoch)\n",
    "            print('Loss:')\n",
    "            print(loss)\n",
    "\n",
    "            # zero gradients, perform a backward pass, and update the weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculate Loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        net.eval()\n",
    "        valid_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(int(valid_data.img_count / batch_size)):\n",
    "                \n",
    "                valid_data.next_batch()\n",
    "                \n",
    "                inputs = valid_data.inputs\n",
    "                inputs = np.transpose(inputs, (0, 3, 1, 2))\n",
    "                inputs.astype(np.float32)\n",
    "                inputs = torch.from_numpy(inputs)\n",
    "\n",
    "                outputs = torch.from_numpy(valid_data.outputs)\n",
    "                outputs = np.transpose(outputs, (0, 3, 1, 2))\n",
    "\n",
    "                inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "                \n",
    "                inputs = inputs.type(torch.float32)\n",
    "                outputs = outputs.type(torch.float32)\n",
    "\n",
    "                y_pred = net(inputs)\n",
    "\n",
    "                # compute and print loss\n",
    "                loss = criterion(y_pred, outputs)\n",
    "\n",
    "                writer.add_scalar('Loss/validate', loss, epoch)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "        t_loss = train_loss / (int(train_data.img_count / batch_size))\n",
    "        v_loss = valid_loss / (int(valid_data.img_count / batch_size))\n",
    "\n",
    "        print(f'Epoch {epoch} \\t\\t Train Loss: {t_loss} \\t\\t Valid Loss: {v_loss}')\n",
    "        print(f'Validation Loss {valid_loss:.6f}) \\t Saving The Model')\n",
    "        \n",
    "        # save 'state dict'\n",
    "        train = train_loss / (int(train_data.img_count / batch_size))\n",
    "        valid = valid_loss / (int(valid_data.img_count / batch_size))\n",
    "        \n",
    "        torch.save(\n",
    "            net.state_dict(), \n",
    "            '../train_results/epoch_' \\ \n",
    "            + str(epoch) \\ \n",
    "            + '_train-loss_' \\\n",
    "            + str(train) \\\n",
    "            + '_val_loss_' \\\n",
    "            + str(valid) \\ \n",
    "            + '.pth'\n",
    "        )\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
